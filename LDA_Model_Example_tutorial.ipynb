{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm+bsIx+uUH0NeVT5OGgVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luis-Alves2/LDA-Tutorial/blob/main/LDA_Model_Example_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Imports\n"
      ],
      "metadata": {
        "id": "KvpTY-mJAPMK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkA8ynTt7mtw",
        "outputId": "425b3cc2-607d-4305-fc5f-6ad2a9236e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.10/dist-packages (6.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "!pip install smart_open nltk gensim\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os.path\n",
        "import re\n",
        "import tarfile\n",
        "import smart_open\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim.models import Phrases\n",
        "from gensim.corpora import Dictionary\n"
      ],
      "metadata": {
        "id": "zP8N7sSiAMkZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qCneV3JDv_J",
        "outputId": "bc8e7d9c-b1ca-402b-a907-0f449b09aca6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
      ],
      "metadata": {
        "id": "hA6aVRvjAlFb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definindo Corpus, com Docs extraidos de link( Tutorial)"
      ],
      "metadata": {
        "id": "1MiIIuuEARM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
        "    with smart_open.open(url, \"rb\") as file:\n",
        "        with tarfile.open(fileobj=file) as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
        "                    member_bytes = tar.extractfile(member).read()\n",
        "                    yield member_bytes.decode('utf-8', errors='replace')"
      ],
      "metadata": {
        "id": "X8EkgWwPAntW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = list(extract_documents())"
      ],
      "metadata": {
        "id": "SMjwMYJFAoIh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(docs[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7YDU90Ay5Z",
        "outputId": "985ffe6c-1284-4b78-f0ee-048e36f79437"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1740\n",
            "387 \n",
            "Neural Net and Traditional Classifiers  \n",
            "William Y. Huang and Richard P. Lippmann \n",
            "MIT Lincoln Laboratory \n",
            "Lexington, MA 02173, USA \n",
            "Abstract\n",
            "Previous work on nets with continuous-valued inputs led to generative \n",
            "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
            "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
            "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
            "can form both c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorize e pre process\n"
      ],
      "metadata": {
        "id": "0UojFFBhA1Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the documents into tokens.\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for idx in range(len(docs)):\n",
        "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
        "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
        "\n",
        "# Remove numbers, but not words that contain numbers.\n",
        "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
        "\n",
        "# Remove words that are only one character.\n",
        "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
      ],
      "metadata": {
        "id": "5TvoZA21Bhiz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the documents.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
      ],
      "metadata": {
        "id": "Kkk678zgCAh6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute bigrams.\n",
        "from gensim.models import Phrases\n",
        "\n",
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(docs, min_count=20)\n",
        "for idx in range(len(docs)):\n",
        "    for token in bigram[docs[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            docs[idx].append(token)"
      ],
      "metadata": {
        "id": "CwjJ9VCoCOfG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)"
      ],
      "metadata": {
        "id": "TU_CDTJlCU63"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words"
      ],
      "metadata": {
        "id": "s2V247FzDX3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]"
      ],
      "metadata": {
        "id": "DK4mqIcBDZR1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr1IcOmVDakD",
        "outputId": "bb049408-b6c7-4656-ce67-70c2afd00014"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 8644\n",
            "Number of documents: 1740\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training"
      ],
      "metadata": {
        "id": "ygefTUErGDBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LDA model.\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make an index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")"
      ],
      "metadata": {
        "id": "1oS7a9VCGEGC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_topics = model.top_topics(corpus)\n",
        "\n",
        "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
        "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
        "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(top_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nKByzb-HItp",
        "outputId": "bb11cb3e-ec8e-407b-ae96-01a12426f479"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average topic coherence: -1.1600.\n",
            "[([(0.012045064, 'stimulus'),\n",
            "   (0.010159217, 'visual'),\n",
            "   (0.010046919, 'response'),\n",
            "   (0.008278891, 'cell'),\n",
            "   (0.007922317, 'orientation'),\n",
            "   (0.007790254, 'field'),\n",
            "   (0.00776493, 'activity'),\n",
            "   (0.006778454, 'cortex'),\n",
            "   (0.005776343, 'receptive'),\n",
            "   (0.005659936, 'cortical'),\n",
            "   (0.005468552, 'frequency'),\n",
            "   (0.0054076924, 'correlation'),\n",
            "   (0.0053951223, 'spatial'),\n",
            "   (0.005347881, 'receptive_field'),\n",
            "   (0.0046996027, 'signal'),\n",
            "   (0.004194944, 'connection'),\n",
            "   (0.0040912535, 'map'),\n",
            "   (0.004084548, 'neuron'),\n",
            "   (0.0038993047, 'noise'),\n",
            "   (0.0038398877, 'center')],\n",
            "  -0.7962247016213089),\n",
            " ([(0.009568345, 'memory'),\n",
            "   (0.0087028025, 'neuron'),\n",
            "   (0.0071405377, 'net'),\n",
            "   (0.0065950584, 'layer'),\n",
            "   (0.005421945, 'node'),\n",
            "   (0.0053603724, 'dynamic'),\n",
            "   (0.005356251, 'recurrent'),\n",
            "   (0.0046124146, 'connection'),\n",
            "   (0.0037603115, 'activation'),\n",
            "   (0.0036677395, 'sequence'),\n",
            "   (0.0036129758, 'architecture'),\n",
            "   (0.003601215, 'attractor'),\n",
            "   (0.003580478, 'signal'),\n",
            "   (0.0035245756, 'propagation'),\n",
            "   (0.003471188, 'noise'),\n",
            "   (0.0034494658, 'hidden'),\n",
            "   (0.0034462868, 'matrix'),\n",
            "   (0.0032296549, 'bit'),\n",
            "   (0.0032212534, 'gradient'),\n",
            "   (0.0030641158, 'back')],\n",
            "  -0.9929618629812424),\n",
            " ([(0.024400782, 'neuron'),\n",
            "   (0.019058889, 'cell'),\n",
            "   (0.0098017715, 'spike'),\n",
            "   (0.008474746, 'synaptic'),\n",
            "   (0.0072389315, 'firing'),\n",
            "   (0.0065509654, 'signal'),\n",
            "   (0.006292216, 'response'),\n",
            "   (0.0057368963, 'circuit'),\n",
            "   (0.0052673616, 'activity'),\n",
            "   (0.0051027318, 'voltage'),\n",
            "   (0.004948475, 'synapsis'),\n",
            "   (0.004776761, 'frequency'),\n",
            "   (0.004775675, 'potential'),\n",
            "   (0.0039700624, 'channel'),\n",
            "   (0.003923356, 'fig'),\n",
            "   (0.0037449228, 'connection'),\n",
            "   (0.0036312283, 'synapse'),\n",
            "   (0.0034308475, 'stimulus'),\n",
            "   (0.0033292952, 'membrane'),\n",
            "   (0.0033140322, 'threshold')],\n",
            "  -0.995862701378071),\n",
            " ([(0.009216119, 'matrix'),\n",
            "   (0.008162709, 'image'),\n",
            "   (0.0073262183, 'distance'),\n",
            "   (0.0067699775, 'component'),\n",
            "   (0.005013141, 'constraint'),\n",
            "   (0.005006083, 'dimensional'),\n",
            "   (0.0046895803, 'source'),\n",
            "   (0.0046837316, 'basis'),\n",
            "   (0.0043937042, 'transformation'),\n",
            "   (0.0036945185, 'signal'),\n",
            "   (0.0035325927, 'gradient'),\n",
            "   (0.0034978443, 'approximation'),\n",
            "   (0.0034736039, 'solution'),\n",
            "   (0.0032236867, 'noise'),\n",
            "   (0.003206716, 'independent'),\n",
            "   (0.00318055, 'map'),\n",
            "   (0.0031273572, 'pca'),\n",
            "   (0.0030965153, 'gaussian'),\n",
            "   (0.003026498, 'principal'),\n",
            "   (0.002830165, 'rule')],\n",
            "  -1.0885145038703137),\n",
            " ([(0.006605558, 'class'),\n",
            "   (0.0065716878, 'gaussian'),\n",
            "   (0.005827852, 'estimate'),\n",
            "   (0.005687649, 'likelihood'),\n",
            "   (0.005600196, 'mixture'),\n",
            "   (0.005342425, 'density'),\n",
            "   (0.0051077576, 'sample'),\n",
            "   (0.0048128865, 'prior'),\n",
            "   (0.0045663365, 'log'),\n",
            "   (0.0045307847, 'bayesian'),\n",
            "   (0.0039091553, 'posterior'),\n",
            "   (0.0038802796, 'estimation'),\n",
            "   (0.0038314934, 'kernel'),\n",
            "   (0.0036492615, 'regression'),\n",
            "   (0.0036212092, 'em'),\n",
            "   (0.0035698055, 'prediction'),\n",
            "   (0.0034261472, 'xi'),\n",
            "   (0.0033627164, 'classification'),\n",
            "   (0.0032672267, 'markov'),\n",
            "   (0.0031680197, 'hidden')],\n",
            "  -1.0943879069310878),\n",
            " ([(0.010420784, 'recognition'),\n",
            "   (0.010419546, 'word'),\n",
            "   (0.008932338, 'hidden'),\n",
            "   (0.007985976, 'speech'),\n",
            "   (0.0075236727, 'layer'),\n",
            "   (0.00710853, 'net'),\n",
            "   (0.0066440334, 'trained'),\n",
            "   (0.005441255, 'character'),\n",
            "   (0.0046804314, 'hidden_unit'),\n",
            "   (0.0045795576, 'architecture'),\n",
            "   (0.003607436, 'sequence'),\n",
            "   (0.003465239, 'human'),\n",
            "   (0.0033744026, 'classification'),\n",
            "   (0.003331999, 'training_set'),\n",
            "   (0.003177824, 'table'),\n",
            "   (0.0030005446, 'letter'),\n",
            "   (0.0029428988, 'signal'),\n",
            "   (0.0027713436, 'digit'),\n",
            "   (0.002757837, 'context'),\n",
            "   (0.0027464582, 'connectionist')],\n",
            "  -1.1089604318145823),\n",
            " ([(0.0072772754, 'rule'),\n",
            "   (0.006505203, 'tree'),\n",
            "   (0.005575098, 'classifier'),\n",
            "   (0.005347109, 'hidden'),\n",
            "   (0.005284913, 'node'),\n",
            "   (0.0052801073, 'class'),\n",
            "   (0.0050240615, 'expert'),\n",
            "   (0.0041141766, 'classification'),\n",
            "   (0.0036716654, 'hidden_unit'),\n",
            "   (0.0035455627, 'search'),\n",
            "   (0.0035268087, 'cluster'),\n",
            "   (0.0030145484, 'decision'),\n",
            "   (0.0029999563, 'region'),\n",
            "   (0.0028381257, 'graph'),\n",
            "   (0.002809034, 'cost'),\n",
            "   (0.00278583, 'generalization'),\n",
            "   (0.0027321856, 'solution'),\n",
            "   (0.002521556, 'table'),\n",
            "   (0.0025074687, 'layer'),\n",
            "   (0.0024983303, 'clustering')],\n",
            "  -1.1290907401477386),\n",
            " ([(0.028492386, 'image'),\n",
            "   (0.014999297, 'object'),\n",
            "   (0.010599536, 'motion'),\n",
            "   (0.009014944, 'visual'),\n",
            "   (0.007610622, 'direction'),\n",
            "   (0.0071641407, 'position'),\n",
            "   (0.006476253, 'field'),\n",
            "   (0.0064314646, 'face'),\n",
            "   (0.006394141, 'pixel'),\n",
            "   (0.0059879883, 'map'),\n",
            "   (0.005395461, 'eye'),\n",
            "   (0.005203396, 'recognition'),\n",
            "   (0.0051323026, 'velocity'),\n",
            "   (0.0050841086, 'view'),\n",
            "   (0.0045006135, 'location'),\n",
            "   (0.004306265, 'movement'),\n",
            "   (0.00409318, 'vision'),\n",
            "   (0.0037181543, 'layer'),\n",
            "   (0.0036790709, 'head'),\n",
            "   (0.0034124025, 'motor')],\n",
            "  -1.2844426326510774),\n",
            " ([(0.0131218955, 'action'),\n",
            "   (0.010013509, 'policy'),\n",
            "   (0.008344015, 'reinforcement'),\n",
            "   (0.007321135, 'chip'),\n",
            "   (0.006879874, 'circuit'),\n",
            "   (0.006390214, 'analog'),\n",
            "   (0.0063767396, 'control'),\n",
            "   (0.0056772926, 'reinforcement_learning'),\n",
            "   (0.005274958, 'optimal'),\n",
            "   (0.0046433886, 'reward'),\n",
            "   (0.0045037055, 'environment'),\n",
            "   (0.0040527903, 'gate'),\n",
            "   (0.0034874843, 'goal'),\n",
            "   (0.0034564782, 'robot'),\n",
            "   (0.0033444352, 'dynamic'),\n",
            "   (0.0033006854, 'sutton'),\n",
            "   (0.0032628232, 'decision'),\n",
            "   (0.003223826, 'agent'),\n",
            "   (0.0032147614, 'threshold'),\n",
            "   (0.002846439, 'machine')],\n",
            "  -1.4423840516703141),\n",
            " ([(0.007539304, 'control'),\n",
            "   (0.007339354, 'bound'),\n",
            "   (0.006906633, 'generalization'),\n",
            "   (0.005736058, 'optimal'),\n",
            "   (0.005663881, 'dynamic'),\n",
            "   (0.005319826, 'theorem'),\n",
            "   (0.0049224063, 'trajectory'),\n",
            "   (0.004750475, 'approximation'),\n",
            "   (0.0046484405, 'let'),\n",
            "   (0.004105678, 'dimension'),\n",
            "   (0.0039595556, 'hidden'),\n",
            "   (0.0035815304, 'field'),\n",
            "   (0.0035446777, 'controller'),\n",
            "   (0.0033418334, 'noise'),\n",
            "   (0.003307364, 'solution'),\n",
            "   (0.0032098524, 'generalization_error'),\n",
            "   (0.0031773055, 'teacher'),\n",
            "   (0.0029175875, 'vc'),\n",
            "   (0.002886256, 'student'),\n",
            "   (0.0028529288, 'curve')],\n",
            "  -1.6667110920812676)]\n"
          ]
        }
      ]
    }
  ]
}